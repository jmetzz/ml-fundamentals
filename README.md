## Preamble
 
This repository contains the code used in *JArchitect's Tech Day* team event (April/2018).
Here you will find simple raw implementation of some well known **fundamental** machine learning algorithms,
which are useful for **educational purposes only**. Thus, please look around and take bits and snippets,
but by no means use them in production code. They are not designed/coded with performance and/or good practices in mind.

Also, bear in mind this repository is **working in progress** and might change considerably in the near future.

By the way, I might be using a bit of everything to show these *fundamental algorithms*.
You might find code written in python, R, java, kotlin or ... God only knows.
There is no guarantee that any code presented here works at all.
Please contribute with suggestions and pull requests if you find any weak spot and bug.

## A brief description of the technical session:

**Title**: Diving in the fundamental machine learning algorithms: let's code them!

**Abstract**: Supervised Learning, Reinforcement Learning, Linear Regression, Neural Networks, Evolutionary Algorithms,
Deep Learning and so many others. Are these all buzzwords? Nowadays everybody have heard or read about these terms somewhere,
but seldom know how they really work. The reason is quite simple: people don't know the basic concepts on which 
the most powerful algorithms are built upon. Therefore, let's shed some light upon the subject by implementing 
a few of the fundamental machine learning algorithms, and hopefully be able to grasp the basics to help us understand
the more advanced techniques and algorithms later on. Let's start with non-supervised and supervised learning, such as:

  - K-means
  - Hierarchical clustering (Single Linkage)
  - K-nearest neighbour (KNN);
  - Naive Bayes;
  - Logistic Regression;
  - CART (Gini index and entropy based); 
  - Perceptron and Feedforward Multi-layer Perceptron (MLP); and
  - Confusion matrix and basic evaluation metrics
  
  
## Data

For most examples in this repository I'm using example datasets from the UCI repository.

    Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. 
    Irvine, CA: University of California, School of Information and Computer Science.

Oftentimes there will be examples using simpler datasets which are hard-coded into the scripts or 
even randomly generated for that specific scenario.
 

  
## Future
I intend to add some content in the future, which will possibly cover the following topics:
  
  - More algorithm's evaluation metrics;
  - Experiment's setup and execution;
  - More Hierarchical clustering (Complete Linkage, Ward ...);
  - Expectation-maximization;
  - Improvements on Decision Trees;
  - Back-propagation Multi-layer Perceptron;
  - Genetic Algorithms;
  - Ensemble Methods (AdaBoost and alike);
  - Support Vector Machines; and
  - Deep Learning.





